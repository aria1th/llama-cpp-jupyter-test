{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-cpp-python --upgrade\n",
    "%pip install transformers --upgrade\n",
    "%pip install llama-cpp-python[server]\n",
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install pygithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import git\n",
    "import subprocess\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../llama.cpp\"):\n",
    "    git.Repo.clone_from(\"https://github.com/ggerganov/llama.cpp\", \"../llama.cpp\")\n",
    "assert os.path.exists(\"../llama-cpp\"), \"llama-cpp not found\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads the model. See https://github.com/abetlen/llama-cpp-python for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model and locate at ./models/gpt4all/gpt4all-lora-quantized.bin\n",
    "# path = https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin\n",
    "# if file exists, skip this step\n",
    "# !wget https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin -P ./models/gpt4all/\n",
    "\n",
    "def resolve_huggingface(huggingface_url):\n",
    "    'https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/blob/main/ggml-alpaca-7b-q4.bin' # convert to\n",
    "    'https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/resolve/main/ggml-alpaca-7b-q4.bin'\n",
    "    if 'blob' in huggingface_url:\n",
    "        huggingface_url = huggingface_url.replace('blob', 'resolve')\n",
    "        return huggingface_url\n",
    "    return huggingface_url\n",
    "\n",
    "def download_model(model_path, model_url):\n",
    "    subprocess.run([\"wget\", \"-nc\",\"-c\", \"-P\", model_path, model_url])\n",
    "    # -nc: skip if file exists\n",
    "    # -P: path to save the file\n",
    "    # -c: continue download if interrupted\n",
    "\n",
    "def download_gpt4all():\n",
    "    model_path = \"./models/gpt4all/\"\n",
    "    model_name = \"gpt4all-lora-quantized.bin\"\n",
    "    model_url = \"https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin\"\n",
    "    if not os.path.exists(model_path + new_model_name):\n",
    "        download_model(model_path, model_url)\n",
    "    else:\n",
    "        print(\"skipping download, model already exists\")\n",
    "        return\n",
    "    # Using subprocess, execute llama.cpp convert.py to convert the model to .bin\n",
    "    #python3 convert.py ./models/gpt4all/gpt4all-lora-quantized.bin\n",
    "    # result is ggml-model-q4_0.bin\n",
    "    new_model_name = 'ggml-model-q4_0.bin'\n",
    "    if not os.path.exists(model_path + new_model_name):\n",
    "        print(\"Downloaded the model, now converting...\")\n",
    "        # clone ./models/added-tokens.json and tokenizer.model to ../llama.cpp/models\n",
    "        if not os.path.exists(\"../llama.cpp/models/added_tokens.json\"):\n",
    "            subprocess.run([\"cp\", \"./models/added_tokens.json\", \"../llama.cpp/models/\"])\n",
    "        if not os.path.exists(\"../llama.cpp/models/tokenizer.model\"):\n",
    "            subprocess.run([\"cp\", \"./models/tokenizer.model\", \"../llama.cpp/models/\"])\n",
    "        subprocess.run([\"python\", \"../llama.cpp/convert.py\", model_path + model_name])\n",
    "    # remove the original model\n",
    "    subprocess.run([\"rm\", model_path + model_name])\n",
    "    return model_path + new_model_name\n",
    "\n",
    "def download_alpaca():\n",
    "    # alpaca does not work currently\n",
    "    model_url = 'https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/resolve/main/ggml-alpaca-7b-q4.bin'\n",
    "    model_path = \"./models/alpaca/\"\n",
    "    model_name = \"ggml-alpaca-7b-q4.bin\"\n",
    "    download_model(model_path, model_url)\n",
    "    return model_path + model_name\n",
    "\n",
    "def download_llama():\n",
    "    # we can't download llama, but we assume the model is directly uploaded to the server\n",
    "    model_path = \"./models/llama/\"\n",
    "    model_name = \"ggml-model-q4_0.bin\"  # quantized 4bit model\n",
    "    return model_path + model_name\n",
    "    \n",
    "\n",
    "abs_model_path = download_gpt4all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = Llama(model_path=abs_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"Q: What is the meaning of life?\"\n",
    "#llm(question, max_tokens=64, stop=[\"Q:\", \"\\n\"], echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the following in terminal, for safety I won't run it here. Process may not be killed properly with ipython.\n",
    "# Model path can be customized.\n",
    "# export MODEL=/workspaces/codespaces-jupyter/notebooks/models/gpt4all/ggml-model-q4_0.bin\n",
    "# python -m llama_cpp.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # can be anything\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:8000/v1\"\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "# server_host = \"http://localhost:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms(\n",
    "    prompt=\"Dr.pepper is \",\n",
    "    stop=[\".\", \"\\n\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
