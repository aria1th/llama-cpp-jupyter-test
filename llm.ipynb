{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-cpp-python --upgrade\n",
    "%pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads the model. See https://github.com/abetlen/llama-cpp-python for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model and locate at ./models/gpt4all/gpt4all-lora-quantized.bin\n",
    "# path = https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin\n",
    "# if file exists, skip this step\n",
    "# !wget https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin -P ./models/gpt4all/\n",
    "import subprocess\n",
    "import os\n",
    "def resolve_huggingface(huggingface_url):\n",
    "    'https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/blob/main/ggml-alpaca-7b-q4.bin' # convert to\n",
    "    'https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/resolve/main/ggml-alpaca-7b-q4.bin'\n",
    "    if 'blob' in huggingface_url:\n",
    "        huggingface_url = huggingface_url.replace('blob', 'resolve')\n",
    "        return huggingface_url\n",
    "    return huggingface_url\n",
    "\n",
    "def download_model(model_path, model_url):\n",
    "    subprocess.run([\"wget\", \"-nc\",\"-c\", \"-P\", model_path, model_url])\n",
    "    # -nc: skip if file exists\n",
    "    # -P: path to save the file\n",
    "    # -c: continue download if interrupted\n",
    "\n",
    "def download_gpt4all():\n",
    "    model_path = \"./models/gpt4all/\"\n",
    "    model_name = \"gpt4all-lora-quantized.bin\"\n",
    "    model_url = \"https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin\"\n",
    "    download_model(model_path, model_url)\n",
    "    # Using subprocess, execute llama.cpp convert.py to convert the model to .bin\n",
    "    #python3 convert.py ./models/gpt4all/gpt4all-lora-quantized.bin\n",
    "    # result is ggml-model-q4_0.bin\n",
    "    new_model_name = 'ggml-model-q4_0.bin'\n",
    "    if not os.path.exists(model_path + new_model_name):\n",
    "        print(\"Downloaded the model, now converting...\")\n",
    "        # clone ./models/added-tokens.json and tokenizer.model to ../llama.cpp/models\n",
    "        if not os.path.exists(\"../llama.cpp/models/added_tokens.json\"):\n",
    "            subprocess.run([\"cp\", \"./models/added_tokens.json\", \"../llama.cpp/models/\"])\n",
    "        if not os.path.exists(\"../llama.cpp/models/tokenizer.model\"):\n",
    "            subprocess.run([\"cp\", \"./models/tokenizer.model\", \"../llama.cpp/models/\"])\n",
    "        subprocess.run([\"python\", \"../llama.cpp/convert.py\", model_path + model_name])\n",
    "    return model_path + new_model_name\n",
    "\n",
    "def download_alpaca():\n",
    "    # alpaca does not work currently\n",
    "    model_url = 'https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/resolve/main/ggml-alpaca-7b-q4.bin'\n",
    "    model_path = \"./models/alpaca/\"\n",
    "    model_name = \"ggml-alpaca-7b-q4.bin\"\n",
    "    download_model(model_path, model_url)\n",
    "    return model_path + model_name\n",
    "\n",
    "def download_llama():\n",
    "    # we can't download llama, but we assume the model is directly uploaded to the server\n",
    "    model_path = \"./models/llama/\"\n",
    "    model_name = \"ggml-model-q4_0.bin\"  # quantized 4bit model\n",
    "    return model_path + model_name\n",
    "    \n",
    "\n",
    "abs_model_path = download_gpt4all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(model_path=abs_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Q: What does the following code do? explain with reason : ```((lambda:print)())((lambda x:[itertools:=__import__('itertools'),lst:=[x],lst.extend(list(itertools.takewhile(lambda a:a!=1,(x:=(lambda:x//2,lambda:3*x+1)[x&1]() for _ in itertools.count())))),lst.append(1)][1])(63))``` A: \"\n",
    "llm(question, max_tokens=64, stop=[\"Q:\", \"\\n\"], echo=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
